---
title: "Big O"
date: 2024-12-13T16:56:59+13:00
draft: false
---

BO important details of an algorithm

3x2+x+1 o(n2)
look for the biggest term
carries most of the weight of the algorithm

Complexity O(n) - goes through all the items in the loops 
eg for loop v size of input array
n is length of the array

no loops constant 

big o ignores coeficient just looking for loops

```
function find(needle, haystack){
    for (var i=0; i < haystack.length; i++) {
        if (haystack === needle) return true;
    }   
}

```
still o(n)
worst case scenario - last item in array
best case scenario - first item in the array
bigO doesnt care 


pair of values 
nested loops
o(n) --> Linear 
o(n2)  ----> n squared (Quadratic Time)
o(n3)  ----> n cubed -NO 
```
makeTuples(input){
    var answer = [];
      for (var i=0; i < input.length; i++) {
        for (var j=0; j < input.length; j++) {
            answer.push(input[i], input[j]) 
        } 
      } 
      return answer;
}

```
algorithms - designed different problems 

trade offs better v worse complexity  

```
function getMiddleOfArray(array){
    return array[Math.floor(array.length/2)]
}
```
Constant Time 
o(1) - doesnt matter how long the array is still takes the same amount of time to do 

graph 
(y) how long it takes the fucntion to run 
(x) number of items in the array

Logrithmic time (1000 v 100 items not that much more time)
algorithm to divide into chunks

more inputs will it take longer not just for loops 

Question --> it depends i need more information 
ie how big is the system 

make more maintainable code v making faster

science of trade offs

- one tool is big O

Complete picture of what the requirements are 

end user 
constraints 
devices

time (computational complexity) - how long it takes to run  
space complexity (space)



